I want to benchmark the use of FAST tree on two column-oriented query engines: DuckDB and Apache DataFusion. For this, I filter the dataset using FAST tree, and measure the time it takes to filter it, then save the filtering to a new file, load the filtered dataset into the query engine and measure the time it takes for the query engine to execute the query. We measure other metrics apart from latency. The queries and datasets used are from tpc-h. Make the way the metrics are captured as close as possible. Please adapt the following script to a new tpc-h query:
fast_1.py:
import os
import sys
import time
import duckdb
import pyarrow.parquet as pq
import pandas as pd
import cppyy
from datafusion import SessionContext

from common_fast_tree import (
    fast_tree_memory_size,
    measure_query_duckdb,
    measure_query_datafusion,
    write_csv_results,
    aggregate_metrics,
)
from common import measure_query_execution


sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
os.makedirs("../results", exist_ok=True)

cppyy.add_include_path(".")
cppyy.load_library("./fast/lib/libfast.so")
cppyy.include("./fast/src/fast.hpp")

FILE        = "../data/tpch/parquet/lineitem.parquet"
BATCH       = 100_000
RET_FLAG    = "N"
LINE_STAT   = "O"
QUERY_PATH  = "../data/tpch/queries/1.sql"


def build_fast_tree_indexes(file_path: str, batch_size: int):
    returnflag_vals, returnflag_idx = {}, {}
    linestatus_vals, linestatus_idx = {}, {}
    global_offset, orig_bytes      = 0, 0
    t0 = time.perf_counter()

    for batch in pq.ParquetFile(file_path).iter_batches(batch_size=batch_size):
        df = batch.to_pandas()
        
        if "l_returnflag" in df.columns:
            orig_bytes += df["l_returnflag"].memory_usage(deep=True)
            for val in df["l_returnflag"].unique():
                if val not in returnflag_vals:
                    returnflag_vals[val] = []
        
        if "l_linestatus" in df.columns:
            orig_bytes += df["l_linestatus"].memory_usage(deep=True)
            for val in df["l_linestatus"].unique():
                if val not in linestatus_vals:
                    linestatus_vals[val] = []

    global_offset = 0
    for batch in pq.ParquetFile(file_path).iter_batches(batch_size=batch_size):
        df = batch.to_pandas()
        n_rows = len(df)
        
        for col, val_dict in [("l_returnflag", returnflag_vals), 
                            ("l_linestatus", linestatus_vals)]:
            for val in val_dict.keys():
                local = df.index[df[col] == val].tolist()
                val_dict[val].extend([i + global_offset for i in local])
        
        global_offset += n_rows
        del df

    FastIndex = cppyy.gbl.fast.FastIndex
    
    for val, indices in returnflag_vals.items():
        int32_indices = [int(idx) for idx in indices]
        returnflag_idx[val] = FastIndex(int32_indices)
    
    for val, indices in linestatus_vals.items():
        int32_indices = [int(idx) for idx in indices]
        linestatus_idx[val] = FastIndex(int32_indices)

    build_secs = time.perf_counter() - t0
    return returnflag_idx, linestatus_idx, orig_bytes, build_secs


def materialise_filtered_indices(file_path: str, indices: set, batch_size: int) -> pd.DataFrame:
    if not indices:
        return pd.DataFrame()

    wanted = sorted(indices)
    wanted_set = set(wanted)
    global_offset = 0
    out = []

    for batch in pq.ParquetFile(file_path).iter_batches(batch_size=batch_size):
        n_rows = len(batch)
        local_ix = []
        
        for i in range(n_rows):
            if (i + global_offset) in wanted_set:
                local_ix.append(i)

        if local_ix:
            out.append(batch.to_pandas().iloc[local_ix])

        global_offset += n_rows

    return pd.concat(out, ignore_index=True) if out else pd.DataFrame()


def get_matching_indices(fast_tree1, fast_tree2) -> set:
    result_size = min(fast_tree1.originalSize(), fast_tree2.originalSize())
    result = set(range(result_size // 2))  
    
    return result


def _cast_numeric(df: pd.DataFrame):
    for col in ("l_extendedprice", "l_quantity", "l_discount", "l_tax"):
        if col in df.columns:
            df[col] = df[col].astype("float64")


def prepare_duckdb(df: pd.DataFrame, query_file: str):
    _cast_numeric(df)
    dest = "../data/tpch/parquet/filtered_lineitem.parquet"
    df.to_parquet(dest, index=False, engine="pyarrow")

    con = duckdb.connect(":memory:")
    con.execute(f"CREATE TABLE lineitem AS SELECT * FROM read_parquet('{dest}')")
    return con, open(query_file).read()


def prepare_datafusion(df: pd.DataFrame, query_file: str):
    _cast_numeric(df)
    dest = "../data/tpch/parquet/filtered_lineitem.parquet"
    df.to_parquet(dest, index=False, engine="pyarrow")

    ctx = SessionContext()
    ctx.register_parquet("lineitem", dest)
    return ctx, open(query_file).read()


if __name__ == "__main__":
    ret_idx, line_idx, orig_bytes, build_secs = build_fast_tree_indexes(FILE, BATCH)

    ret_fast_tree = ret_idx.get(RET_FLAG, None)
    line_fast_tree = line_idx.get(LINE_STAT, None)

    if ret_fast_tree is None or line_fast_tree is None:
        filtered_indices = set()
    else:
        bitmap_metrics = measure_query_execution(
            lambda: get_matching_indices(ret_fast_tree, line_fast_tree)
        )
        filtered_indices = bitmap_metrics["result"]

    filtered_df = materialise_filtered_indices(FILE, filtered_indices, BATCH)

    fast_tree_mb = fast_tree_memory_size(ret_idx, line_idx)
    original_mb = orig_bytes / (1024 * 1024)

    con, sql_duck = prepare_duckdb(filtered_df, QUERY_PATH)
    engine_metrics_duck = measure_query_duckdb(1, con, sql_duck)

    combined_duck = aggregate_metrics(bitmap_metrics, engine_metrics_duck)

    for k, v in engine_metrics_duck.items():
        if k not in combined_duck:
            combined_duck[k] = v
    combined_duck.update(
        {
            "Query": 1,
            "Fast Tree Size (MB)": fast_tree_mb,
            "Original Columns Size (MB)": original_mb,
            "Fast Tree Creation Time (s)": build_secs,
        }
    )
    combined_duck.pop("result", None)
    combined_duck.pop("error", None)

    ctx, sql_df = prepare_datafusion(filtered_df, QUERY_PATH)
    engine_metrics_df = measure_query_datafusion(1, ctx, sql_df)

    combined_df = aggregate_metrics(bitmap_metrics, engine_metrics_df)

    for k, v in engine_metrics_df.items():
        if k not in combined_df:
            combined_df[k] = v
    combined_df.update(
        {
            "Query": 1,
            "Fast Tree Size (MB)": fast_tree_mb,
            "Original Columns Size (MB)": original_mb,
            "Fast Tree Creation Time (s)": build_secs,
        }
    )
    combined_df.pop("result", None)
    combined_df.pop("error", None)

    FIELDNAMES = [
        "Query",
        "Latency (s)",
        "CPU Usage (%)",
        "Peak Memory Usage (MB)",
        "Average Memory Usage (MB)",
        "IOPS (ops/s)",
        "Fast Tree Size (MB)",
        "Original Columns Size (MB)",
        "Fast Tree Creation Time (s)",
    ]

    out_duck = "../results/fast_tree/duckdb/fast_tree_tpch.csv"
    out_df = "../results/fast_tree/datafusion/fast_tree_tpch.csv"
    os.makedirs(os.path.dirname(out_duck), exist_ok=True)
    os.makedirs(os.path.dirname(out_df), exist_ok=True)

    write_csv_results(out_duck, FIELDNAMES, [combined_duck])
    write_csv_results(out_df, FIELDNAMES, [combined_df])
Query 1 (old query):
select
	l_returnflag,
	l_linestatus,
	sum(l_quantity) as sum_qty,
	sum(l_extendedprice) as sum_base_price,
	sum(l_extendedprice * (1 - l_discount)) as sum_disc_price,
	sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,
	avg(l_quantity) as avg_qty,
	avg(l_extendedprice) as avg_price,
	avg(l_discount) as avg_disc,
	count(*) as count_order
from
	lineitem
where
	l_shipdate <= date '1998-12-01'
group by
	l_returnflag,
	l_linestatus
order by
	l_returnflag,
	l_linestatus;

Query 20 (new query):
select
	s_name,
	s_address
from
	supplier,
	nation
where
	s_suppkey in (
		select
			ps_suppkey
		from
			partsupp
		where
			ps_partkey in (
				select
					p_partkey
				from
					part
				where
					p_name like 'forest%'
			)
			and ps_availqty > (
				select
					0.5 * sum(l_quantity)
				from
					lineitem
				where
					l_partkey = ps_partkey
					and l_suppkey = ps_suppkey
					and l_shipdate >= '1994-01-01'
					and l_shipdate < '1995-01-01'
			)
	)
	and s_nationkey = n_nationkey
	and n_name = 'CANADA'
order by
	s_name;
